{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll take a look at the BERT annotated data and see if we can do anything with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australian Film Classification Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful resources for how film classifications are determined can be found here: \n",
    "\n",
    "https://www.classification.gov.au/classification-ratings/how-rating-decided\n",
    "\n",
    "Some relevant excerpts below: \n",
    "\n",
    "Approved classification tools use logic rules and algorithms to classify content.\n",
    "\n",
    "The guidelines include 6 classifiable elements. These are:\n",
    "\n",
    "themes\n",
    "violence\n",
    "sex\n",
    "language\n",
    "drug use\n",
    "nudity.\n",
    "\n",
    "Impact\n",
    "The impact depends on the frequency, intensity and the overall effect of the content. The purpose, tone and style can affect impact.\n",
    "\n",
    "Impact may increase where depictions are:\n",
    "\n",
    "detailed\n",
    "prolonged\n",
    "realistic\n",
    "interactive.\n",
    "Impact may be lower where content is:\n",
    "\n",
    "implied rather than depicted\n",
    "not detailed\n",
    "short in duration\n",
    "verbal and not visual\n",
    "incidental and not direct.\n",
    "The level of impact allowed in each classification category (rating):\n",
    "\n",
    "Rating\tImpact level\n",
    "G\tVery mild\n",
    "PG\tMild\n",
    "M\tModerate\n",
    "MA 15+\tStrong\n",
    "R 18+\tHigh\n",
    "RC\tVery high\n",
    "\n",
    "<i> So we have six relevant categories, and six impact levels corresponding to the 6 ratings.  The goal will be then to build a model that basically scores the impact level of the screenplay on these six measures, then uses those six scores to make the prediction/classsification.  So e.g., we might except a neural network architecture to look like:\n",
    "\n",
    "embedding layer(vector for each {sentence, word} in screenplay)\n",
    "- > 6 (?) neurons, one for each classifiable element\n",
    "( -> context layer? )\n",
    "- > 6 (?) neurons, an impact score for each element \n",
    "- > 1 output neuron for classification\n",
    "OR \n",
    "- > an output layer per country classification desired? </i> \n",
    "\n",
    "Context\n",
    "Context determines whether the storyline justifies content. Content that falls into a particular rating in one context may fall outside it in a different context.\n",
    "\n",
    "For example, the way the content deals with social issues may require a mature or adult perspective. Historical context may also justify certain depictions.\n",
    "\n",
    "<i> Should there be a separate context layer then? </i>\n",
    "\n",
    "Consumer advice\n",
    "Under section 20 of the Act, a classification decision must include consumer advice.\n",
    "\n",
    "Consumer advice gives information about the content. It usually describes the classifiable elements with the greatest impact.\n",
    "\n",
    "For example, a film classified PG may have consumer advice of 'Mild violence and coarse language'. This means that the film is PG because the violence and coarse language are mild in impact.\n",
    "\n",
    "<i> Ideally, our model would also therefore output this Consumer Advice </i> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Data and Assemble DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace paths here\n",
    "root_path = r'C:\\Users\\bened\\DataScience\\ANLP\\AT2'\n",
    "\n",
    "import os \n",
    "\n",
    "folder_path = f'{root_path}\\\\BERT_annotations'\n",
    "screenplays_annot = {}\n",
    "\n",
    "# list all files in folder and iterate over them \n",
    "for file_name in os.listdir(folder_path):\n",
    "    # get file_path by joining folder path with file_name\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # ensure path points to an actual file\n",
    "    if os.path.isfile(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            content = f.read()\n",
    "            screenplays_annot[file_name] = content\n",
    "\n",
    "# ensure files were imported correctly by printing a sample of the first ten files \n",
    "i = 0\n",
    "for file_name, content in screenplays_annot.items():\n",
    "    if i == 10:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Example of {file_name}:\\n\")\n",
    "        print(content[:100])\n",
    "        print(\"-\"*50)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_first_lines(dict_list, n):\n",
    "    for idx, d in enumerate(dict_list):\n",
    "        if idx == n:\n",
    "            break\n",
    "        else:\n",
    "            print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_avg_length(series):\n",
    "    avg_length = np.mean([len(d) for d in series])\n",
    "    print(\"Average length:\", avg_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Join with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meta_df = pd.read_csv(f'{root_path}\\\\movie_meta_data.csv')\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at filename format\n",
    "\n",
    "filenames = list(screenplays_annot.keys())\n",
    "print(filenames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames are formatted as movietitle_IMDBid \n",
    "import re\n",
    "\n",
    "filenames = list(screenplays_annot.keys())\n",
    "movie_titles = []\n",
    "ids = []\n",
    "for f in filenames:\n",
    "    # split at first _ to separate title from rest of filename\n",
    "    split = f.split(sep=\"_\")\n",
    "    movie_title = split[0]\n",
    "    id = split[1]\n",
    "    movie_titles.append(movie_title)\n",
    "    ids.append(id)\n",
    "i = 0\n",
    "for title, id in zip(movie_titles, ids):\n",
    "    if i == 10:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Title:\", title, \" ID:\", id)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_df = pd.DataFrame({\n",
    "    'imdbid': ids,\n",
    "    'annot_screenplay': screenplays_annot.values()\n",
    "})\n",
    "screenplays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_df.info())\n",
    "print(meta_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert screenplays imdbid to int\n",
    "screenplays_df['imdbid'] = screenplays_df['imdbid'].astype(int)\n",
    "df = meta_df.merge(screenplays_df, on='imdbid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roxbury_annot = df.at[0, 'annot_screenplay']\n",
    "print(roxbury_annot[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each \\n introduces a new label: data pairing\n",
    "Try to turn this into a json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Format Text Data as JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format screenplay as json\n",
    "## output should look like e.g.:\n",
    "## {\"label\": \"data\", \"label\":\"data\" etc.}\n",
    "\n",
    "import re\n",
    "\n",
    "def format_as_json(screenplay):\n",
    "    # store results as list of key-value pairs\n",
    "    screenplay_data = []\n",
    "    # split screenplays by \\n\n",
    "    lines = screenplay.split(\"\\n\")\n",
    "    # iterate through lines \n",
    "    for line in lines: \n",
    "        # take part of string up to : as label\n",
    "        match = re.search(r':', line)\n",
    "        if match:\n",
    "            # take end of match as cutoff\n",
    "            cutoff = match.end()\n",
    "            label = line[:cutoff-1]\n",
    "            # after cutoff is data \n",
    "            data = line[cutoff+1:]\n",
    "            # store as dict\n",
    "            line_info = {label:data}\n",
    "            # append to list\n",
    "            screenplay_data.append(line_info)\n",
    "    # return list\n",
    "    return screenplay_data\n",
    "\n",
    "# beta test on roxbury \n",
    "roxbury_data = format_as_json(roxbury_annot)\n",
    "print(roxbury_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll apply this logic to the whole corpus to see if labels are the same\n",
    "\n",
    "screenplay_jsons = df['annot_screenplay'].apply(format_as_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplay_jsons[10][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Analyze labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the unique keys (labels)\n",
    "## iterate through list and return set of unique labels\n",
    "unique_labels = {key for d in roxbury_data for key in d.keys()}\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_labels(json):\n",
    "    unique_labels = {key for d in json for key in d.keys()}\n",
    "    return unique_labels\n",
    "\n",
    "unique_labels_series = screenplay_jsons.apply(find_unique_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Find and drop rows where data is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unequal_length = []\n",
    "for series in unique_labels_series:\n",
    "    if len(series) != 4:\n",
    "        unequal_length.append(series)\n",
    "\n",
    "print(unequal_length)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some annotations have only three labels, which is fine, but others appear to be empty, which we should investigate\n",
    "empty_series = []\n",
    "for idx, series in enumerate(unique_labels_series):\n",
    "    if len(series) == 0:\n",
    "        empty_series.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_imdbid = df.loc[empty_series, 'imdbid']\n",
    "df.loc[empty_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_df[screenplays_df['imdbid'].isin(missing_imdbid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the source data, you'll find the .txt files for these screenplays are simply empty.  We'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(empty_series)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplay_jsons.drop(empty_series, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 'scene_heading', 'speaker_heading', 'text' and 'dialog' labels.  Let's look at an example screenplay to see what might be worth removing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'text' is likely to be relevant, e.g. {'text': 'Of random dancers -- gyrating, flirting, making out, drinking.'}\n",
    "- 'scene_heading' is conceivably relevant, e.g. {'scene_heading': 'INT. DANCE CLUBS- QUICK SHOTS - NIGHT'} -- a scene set in a nightclub might predict a higher classification. \n",
    "Let's look at a representative sample of 'text'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through jsons and return data for 'text' label \n",
    "roxbury_texts = [d.get('text') for d in roxbury_data if 'text' in d]\n",
    "print(roxbury_texts[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roxbury_texts[len(roxbury_texts)-100:len(roxbury_texts)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'texts' are most likely relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases we see the same sentences spread over different values, while key is the same.  The function below will find these contiguous values and flatten them into one value.  This will make sentence tokenization more meaningful later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll define a more general function this time that takes a key input\n",
    "def flatten_data(dict_list, key):\n",
    "    flattened_data = []\n",
    "    temp = ''\n",
    "    for d in dict_list:\n",
    "        if key in d:\n",
    "            temp += ' ' + d[key] if temp else d[key]\n",
    "        else:\n",
    "            # if a key other than input is encountered and temp is not empty\n",
    "            if temp:\n",
    "                # append the concatenated string to text list \n",
    "                flattened_data.append({key:temp})\n",
    "                # and reset temp \n",
    "                temp = ''\n",
    "            # append non text dict to list \n",
    "            flattened_data.append(d)\n",
    "    # after loop ends, concatenate what's left in temp if anything\n",
    "    if temp:\n",
    "        flattened_data.append({key:temp})\n",
    "    # and return concatenated list\n",
    "    return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function on the film 'Anonymous'\n",
    "anon = screenplay_jsons[100]\n",
    "anon_text_flattened = flatten_data(anon, 'text')\n",
    "print(anon_text_flattened[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now for dialog\n",
    "anon_dialog_flattened = flatten_data(anon_text_flattened, 'dialog')\n",
    "for i, d in enumerate(anon_dialog_flattened):\n",
    "    if i == 50:\n",
    "        break\n",
    "    else:\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "appears to have worked, so we'll apply all for both 'text' and 'dialog' keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplays_flat_txt = screenplay_jsons.apply(flatten_data, key='text')\n",
    "screenplays_flat = screenplays_flat_txt.apply(flatten_data, key='dialog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_flat[10][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_flat_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speaker_heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'speaker_heading' is likely irrelevant, but let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_headings = [d.get('speaker_heading') for d in roxbury_data if 'speaker_heading' in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'speaker_heading' almost certainly irrelevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop speaker_heading data \n",
    "def decapitate_speakers(json_list):\n",
    "    decapitated = [d for d in json_list if not 'speaker_heading' in d]\n",
    "    return decapitated \n",
    "\n",
    "# # test on roxbury \n",
    "# roxbury_decapitated = decapitate_speakers(roxbury_data)\n",
    "# print(roxbury_decapitated[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique keys in roxbury_decapitated \n",
    "roxbury_decapitated_labels = find_unique_labels(roxbury_decapitated)\n",
    "print(roxbury_decapitated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up RAM \n",
    "del df, filenames, content, meta_df, ids, movie_titles, roxbury_annot, roxbury_data, screenplays_annot, screenplays_df, speaker_headings, unique_labels_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to all texts \n",
    "decapitated_screenplays = screenplays_flat.apply(decapitate_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decapitated_screenplays[10][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: we can likely remove any values:\n",
    "- that are empty \n",
    "- containing ':' -- these seem to be camera directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roxbury_decapitated[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Empty Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty strings\n",
    "\n",
    "import string \n",
    "puncts = set(string.punctuation)\n",
    "\n",
    "def remove_nulls(json_list):\n",
    "    non_nulls = []\n",
    "    for dict in json_list:\n",
    "        valid = True\n",
    "        for val in dict.values():\n",
    "            if val == '' or all(char in puncts for char in val):\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            non_nulls.append(dict)\n",
    "    return non_nulls\n",
    "\n",
    "# # beta test on roxbury \n",
    "# roxbury_nonna = remove_nulls(roxbury_decapitated)\n",
    "# print(roxbury_nonna[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decapitated_screenplays[10][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to all data \n",
    "screenplays_nonna = decapitated_screenplays.apply(remove_nulls)\n",
    "print(screenplays_nonna[10][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for s in screenplays_nonna[10]:\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(s)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that values in allcaps are basically irrelevant. Let's return these and take a look at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^[^a-z]+$')\n",
    "\n",
    "def no_lower(dict_list):\n",
    "    no_lowers = []\n",
    "    for d in dict_list:\n",
    "        for val in d.values():\n",
    "            if re.match(pattern, val):\n",
    "                no_lowers.append(d)\n",
    "    return no_lowers\n",
    "\n",
    "# test on mohicans\n",
    "mohicans = screenplays_nonna[10]\n",
    "mohicans_no_lowers = no_lower(mohicans)\n",
    "for i, j in enumerate(mohicans_no_lowers):\n",
    "    if j == 20:\n",
    "        break\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "potentially relevant info in here, e.g. 'GUN', 'MASSIVE WAR CLUB'.  However we can delete all strings that match 'CUT TO ...' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_nonna[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_cuts(dict_list):\n",
    "    # empty list for filtered dicts\n",
    "    dicts_uncut = []\n",
    "    for d in dict_list:\n",
    "        # if none of the values in the dict match 'CUT'\n",
    "        if all(not re.search(r'CUT', str(val)) for val in d.values()):\n",
    "            # then append to list\n",
    "            dicts_uncut.append(d)\n",
    "    return dicts_uncut \n",
    "\n",
    "# test on mohicans_no_lowers\n",
    "mohicans_uncut = delete_cuts(mohicans_no_lowers)\n",
    "for i, j in enumerate(mohicans_uncut):\n",
    "    if j == 20:\n",
    "        break\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mohicans[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mohicans_uncut = delete_cuts(mohicans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mohicans_uncut[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_nonna[10][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all \n",
    "import numpy as np\n",
    "\n",
    "# average length before filtering\n",
    "avg_length_before = np.mean([len(s) for s in screenplays_nonna])\n",
    "print(\"avg length before:\", avg_length_before)\n",
    "\n",
    "screenplays_uncut = screenplays_nonna.apply(delete_cuts)\n",
    "\n",
    "# average length after filtering\n",
    "avg_length_after = np.mean([len(s) for s in screenplays_uncut])\n",
    "print(\"avg_length_after:\", avg_length_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_uncut[10][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_nonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.loc[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print another random sample to see where we're at \n",
    "change_up = screenplays_uncut[200]\n",
    "for i, s in enumerate(change_up):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll sentence tokenize the values first before removing punctuation marks etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mohicans_uncut[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# try out first on mohicans\n",
    "mohicans_sents = []\n",
    "\n",
    "for d in mohicans_uncut:\n",
    "    # empty dict for storing result \n",
    "    sents_dict = {}\n",
    "    for key, value in d.items():\n",
    "        # # if the value is a list, unpack the list first (needs to be debugged)\n",
    "        # if isinstance(value, list):\n",
    "        #     value = str(value)\n",
    "        #     sents_dict[key] = sent_tokenize(value)\n",
    "        #     mohicans_sents.append(sents_dict)\n",
    "        # else:\n",
    "        sents_dict[key] = sent_tokenize(value)\n",
    "        mohicans_sents.append(sents_dict)\n",
    "\n",
    "for i, j in enumerate(mohicans_sents):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work okay, although now we have to deal with a list of dicts of lists :/ including lists with one sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this again, make it a dict of dicts? With a structure like\n",
    "{\"screenplay\":\n",
    "    {\"label\":\"data\"},\n",
    "    {\"label\":\"data\"},\n",
    "    etc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unneeded variables before sentence tokenization \n",
    "del anon, anon_dialog_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del anon_text_flattened, change_up, decapitated_screenplays, mohicans, mohicans_no_lowers, mohicans_sents, mohicans_uncut, roxbury_decapitated, roxbury_nonna, roxbury_texts, screenplay_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define as a general function\n",
    "def sent_tokenize_dicts(dict_list):\n",
    "\n",
    "    sentence_dicts = []\n",
    "\n",
    "    for d in dict_list:\n",
    "        # empty dict for storing result \n",
    "        sents_dict = {}\n",
    "        for key, value in d.items():\n",
    "            sents_dict[key] = sent_tokenize(value)\n",
    "            sentence_dicts.append(sents_dict)\n",
    "    \n",
    "    return sentence_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all \n",
    "screenplay_sents = screenplays_uncut.apply(sent_tokenize_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_lines(screenplay_sents[50], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_uncut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we're going to encode our labels just to save on memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'scene_heading': 0,\n",
    "    'text': 1,\n",
    "    'dialog': 2\n",
    "}\n",
    "\n",
    "# check how it will work on ten things I hate about you \n",
    "ten_things_sents = screenplay_sents[50]\n",
    "# iterate through dict list\n",
    "ten_things_encoded = []\n",
    "for d in ten_things_sents:\n",
    "    encoded_dict = {np.int8(label_map[key]): value for key, value in d.items()}\n",
    "    ten_things_encoded.append(encoded_dict)\n",
    "\n",
    "print(ten_things_encoded[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define as function and apply all \n",
    "\n",
    "def encode_labels(dict_list):\n",
    "    encoded_list = []\n",
    "    for d in dict_list:\n",
    "        encoded_dict = {np.int8(label_map[key]): value for key, value in d.items()}\n",
    "        encoded_list.append(encoded_dict)\n",
    "    return encoded_list\n",
    "\n",
    "screenplays_encoded = screenplay_sents.apply(encode_labels)\n",
    "print(screenplays_encoded[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove 'EXT' and 'INT' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove all sentences which contain only EXT/INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_location(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [item for item in value if item not in ['EXT.', 'INT.']]\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on ten things \n",
    "roxbury_sents = screenplays_encoded[0]\n",
    "roxbury_unlocated = remove_location(roxbury_sents)\n",
    "print_first_lines(roxbury_unlocated, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplay_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all\n",
    "screenplays_unlocated = screenplays_encoded.apply(remove_location)\n",
    "print(screenplays_unlocated[10][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_lines(screenplays_unlocated[200], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_unlocated[10][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import copy\n",
    "\n",
    "# try out on mohicans \n",
    "mohicans_sents = copy.deepcopy(screenplays_unlocated[10])\n",
    "\n",
    "def word_tokenize_dicts(dict_list):\n",
    "    # iterate through dict list\n",
    "    for d in dict_list:\n",
    "        # iterate through keys and values \n",
    "        for key, value in d.items():\n",
    "            d[key] = [word_tokenize(sent) for sent in value]\n",
    "    return dict_list\n",
    "\n",
    "mohicans_tokenized = word_tokenize_dicts(mohicans_sents)\n",
    "print_first_lines(mohicans_tokenized, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(screenplays_unlocated[10][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunate that we're now dealing with lists of dicts of lists of lists :/  but not sure how to remedy that without losing sentence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all \n",
    "screenplays_tokenized = screenplays_unlocated.apply(word_tokenize_dicts)\n",
    "print_first_lines(screenplays_tokenized[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove strings that contain no letters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mohicans_tokenized = copy.deepcopy(screenplays_tokenized[10])\n",
    "print_first_lines(mohicans_tokenized, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "puncts = list(string.punctuation)\n",
    "print(puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def contains_letters(token):\n",
    "    return bool(re.search(r'[a-zA-Z]', token))\n",
    "\n",
    "def remove_non_letters(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [\n",
    "                [t for t in sentence if contains_letters(t)]\n",
    "                for sentence in value\n",
    "            ]\n",
    "    return dict_list \n",
    "\n",
    "# test on mohicans_tokenized\n",
    "mohicans_alpha = remove_non_letters(mohicans_tokenized)\n",
    "print_first_lines(mohicans_alpha, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mohicans_alpha, mohicans_sents, mohicans_tokenized, roxbury_sents, roxbury_unlocated, screenplays_encoded, screenplays_unlocated, ten_things_encoded, ten_things_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to work so apply all \n",
    "screenplays_alpha = screenplays_tokenized.apply(remove_non_letters)\n",
    "print_first_lines(screenplays_alpha[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sentence boundaries are already marked, we can convert all chars to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [\n",
    "                [w.lower() for w in sentence]\n",
    "                for sentence in value\n",
    "            ]\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = copy.deepcopy(screenplays_alpha[10])\n",
    "print_first_lines(mo, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on mohicans \n",
    "mo_lower = convert_to_lower(mo)\n",
    "print_first_lines(mo_lower, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consideration = copy.deepcopy(screenplays_alpha[25])\n",
    "print_first_lines(consideration, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all\n",
    "screenplays_lower = screenplays_alpha.apply(convert_to_lower)\n",
    "print_first_lines(screenplays_lower[25], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mo, mo_lower, screenplay_jsons, screenplays_alpha, screenplays_tokenized, unique_labels_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove tokens of length 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_single_chars(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [\n",
    "                [w for w in sentence if len(w) > 1]\n",
    "                for sentence in value]\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [[]]}\n",
      "{0: [['nicole', 'holofcener', 'and', 'jeff', 'whit']]}\n",
      "{1: [['based', 'on', 'the', 'book', 'by']]}\n",
      "{0: [['nicole', 'holofcener', 'and', 'jeff', 'whit']]}\n",
      "{2: [['can', 'you', 'ever', 'forgive', 'me'], ['screenplay', 'by', 'nicole', 'holofcener', 'and', 'jeff', 'whitty', 'based', 'on', 'the', 'book', 'can', 'you', 'ever', 'forgive', 'me'], ['by', 'lee', 'israel', 'final', 'shooting', 'script', 'march']]}\n",
      "{0: [['fox', 'searchlight', 'pictures', 'inc']]}\n",
      "{2: [['los', 'angeles', 'ca']]}\n",
      "{0: [['all', 'rights', 'reserved'], ['copyright', 'willow', 'and', 'oak', 'inc.', 'no']]}\n",
      "{0: [['portion', 'of', 'this', 'script', 'may', 'be', 'performed', 'published', 'reproduced']]}\n",
      "{1: [['sold', 'or', 'distributed', 'by', 'any', 'means', 'or', 'quoted', 'or', 'published', 'in', 'any']]}\n"
     ]
    }
   ],
   "source": [
    "# test on consideration \n",
    "consideration = copy.deepcopy(screenplays_lower[25])\n",
    "consideration_poly = cut_single_chars(consideration)\n",
    "print_first_lines(consideration_poly, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll remove empty values after also removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [['written', 'by', 'rhett', 'reese', 'amp', 'paul', 'wernick', 'final', 'shooting', 'script', 'november']]}\n",
      "{1: [['over', 'black'], ['low', 'volume', 'through', 'tinny', 'speaker', 'juice', 'newton', \"'s\", 'angel', 'of', 'the', 'morning']]}\n",
      "{0: [['ext./int'], ['taxi', 'cab', 'morning']]}\n",
      "{1: [['deadpool', 'in', 'full', 'dress', 'reds', 'and', 'mask', 'quietly', 'fidgets', 'in', 'the', 'back', 'seat', 'of', 'taxi', 'cab', 'as', 'it', 'proceeds', 'along', 'city', 'freeway'], ['deadpool', 'adjusts', 'the', 'two', 'katanas', 'strapped', 'to', 'his', 'back'], ['rolls', 'the', 'windows', 'up', 'down', 'up'], ['tries', 'futilely', 'to', 'untwist', 'the', 'seatbelt', 'then', 'lunges', 'forward', 'locking', 'it', 'up'], ['rifles', 'through', 'tourist', 'booklet', 'and', 'tears', 'out', 'haunted', 'segway', 'tour', 'coupon'], ['the', 'cabbie', 'young', 'thin', 'brown', 'glances', 'back', 'and', 'forth', 'from', 'the', 'rear', 'view', 'to', 'the', 'road', 'to', 'the', 'rear', 'view']]}\n",
      "{2: [['kinda', 'lonesome', 'back', 'here']]}\n",
      "{2: [['little', 'help']]}\n",
      "{1: [['the', 'cabbie', 'grabs', 'deadpool', \"'s\", 'hand', 'and', 'pulls', 'him', 'through', 'to', 'the', 'front'], ['deadpool', \"'s\", 'head', 'rests', 'upside', 'down', 'on', 'the', 'bench', 'seat', 'as', 'he', 'maneuvers', 'his', 'legs', 'through'], ['the', 'cabbie', 'turns', 'the', 'helping', 'hand', 'into', 'handshake', 'then', 'turns', 'down', 'the', 'juice']]}\n",
      "{2: [['dopinder']]}\n",
      "{2: [['still', 'upside-down']]}\n",
      "{1: [['pool'], ['deadpool'], ['dopinder', 'is', 'remarkably', 'unaffected', 'by', 'the', 'lunatic', 'in', 'his', 'cab']]}\n"
     ]
    }
   ],
   "source": [
    "# apply all \n",
    "screenplays_poly = screenplays_lower.apply(cut_single_chars)\n",
    "print_first_lines(screenplays_poly[250], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stops = [\n",
    "'fox', 'searchlight', 'pictures', 'inc', 'los', 'angeles', 'ca',\n",
    "'all', 'rights', 'reserved', 'copyright', 'willow', 'and', 'oak', 'inc.', 'no',\n",
    "'portion', 'of', 'this', 'script', 'may', 'be', 'performed', 'published', 'reproduced',\n",
    "'sold', 'or', 'distributed', 'by', 'any', 'means', 'or', 'quoted', 'or', 'published', 'in', 'any',\n",
    "r'ext./int', 'amp', \"'ll\", 'ext', 'int'\n",
    "]\n",
    "\n",
    "for s in extra_stops:\n",
    "    if s not in stops:\n",
    "        stops.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m consideration, consideration_poly, mo, mo_lower, screenplay_jsons, screenplays_alpha\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mo' is not defined"
     ]
    }
   ],
   "source": [
    "del consideration, consideration_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [\n",
    "                [w for w in sentence if w not in stops]\n",
    "                for sentence in value]\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [['written', 'rhett', 'reese', 'paul', 'wernick', 'final', 'shooting', 'november']]}\n",
      "{1: [['black'], ['low', 'volume', 'tinny', 'speaker', 'juice', 'newton', \"'s\", 'angel', 'morning']]}\n",
      "{0: [[], ['taxi', 'cab', 'morning']]}\n",
      "{1: [['deadpool', 'full', 'dress', 'reds', 'mask', 'quietly', 'fidgets', 'back', 'seat', 'taxi', 'cab', 'proceeds', 'along', 'city', 'freeway'], ['deadpool', 'adjusts', 'two', 'katanas', 'strapped', 'back'], ['rolls', 'windows'], ['tries', 'futilely', 'untwist', 'seatbelt', 'lunges', 'forward', 'locking'], ['rifles', 'tourist', 'booklet', 'tears', 'haunted', 'segway', 'tour', 'coupon'], ['cabbie', 'young', 'thin', 'brown', 'glances', 'back', 'forth', 'rear', 'view', 'road', 'rear', 'view']]}\n",
      "{2: [['kinda', 'lonesome', 'back']]}\n",
      "{2: [['little', 'help']]}\n",
      "{1: [['cabbie', 'grabs', 'deadpool', \"'s\", 'hand', 'pulls', 'front'], ['deadpool', \"'s\", 'head', 'rests', 'upside', 'bench', 'seat', 'maneuvers', 'legs'], ['cabbie', 'turns', 'helping', 'hand', 'handshake', 'turns', 'juice']]}\n",
      "{2: [['dopinder']]}\n",
      "{2: [['still', 'upside-down']]}\n",
      "{1: [['pool'], ['deadpool'], ['dopinder', 'remarkably', 'unaffected', 'lunatic', 'cab']]}\n"
     ]
    }
   ],
   "source": [
    "# test on deadpool \n",
    "deadpool = copy.deepcopy(screenplays_poly[250])\n",
    "deadpool_nonstop = remove_stops(deadpool)\n",
    "print_first_lines(deadpool_nonstop, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [['night', 'roxbury']]}\n",
      "{2: [['written', 'steve', 'koren', 'ferrell', 'chris', 'kattan', 'june']]}\n",
      "{0: [['panoramic', 'view', 'sunset']]}\n",
      "{1: [['hear', 'love', 'haddaway', 'night', 'falls', 'partytime', 'begins']]}\n",
      "{0: [['superimpose', 'sunset', 'blvd.', 'pm']]}\n",
      "{0: [['dance', 'clubs', 'night']]}\n",
      "{2: [['coconut', 'teaser', 'palace', 'roxbury', 'tatou', 'etc']]}\n",
      "{0: [['dance', 'clubs-', 'quick', 'shots', 'night']]}\n",
      "{1: [['random', 'dancers', 'gyrating', 'flirting', 'making', 'drinking']]}\n",
      "{0: [['palace', 'night']]}\n"
     ]
    }
   ],
   "source": [
    "# apply all\n",
    "screenplays_nonstop = screenplays_poly.apply(remove_stops)\n",
    "print_first_lines(screenplays_nonstop[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [['last', 'mohicans']]}\n",
      "{2: [['written', 'michael', 'mann', 'christopher', 'crowe']]}\n",
      "{1: [['screen', 'microcosm', 'leaf', 'crystal', 'drops', 'precipitation', 'stone', 'emerald', 'green', 'moss'], [\"'s\", 'landscape', 'miniature'], ['hear', 'forest'], ['distant', 'birds'], ['sound', 'seems', 'reverberate', 'cavern'], ['piece', 'sunlight', 'refracts', 'within', 'drops', 'water', 'paints', 'patch', 'moss', 'yellow'], ['whisper', 'wind', 'joined', 'another', 'sound', 'mixes'], ['distant', 'rustling'], ['gets', 'closer', 'louder'], [\"'s\", 'shallow', 'breathing'], ['gets', 'ominous'], [\"'re\", 'interlopers', 'floor', 'forest', 'something', 'coming']]}\n",
      "{0: [['suddenly', 'moccasined', 'foot']]}\n",
      "{1: [['rockets', 'frame', 'scaring', 'us']]}\n",
      "{0: [['extremely', 'close', 'part', 'indian', 'face']]}\n",
      "{1: [['running', 'hard'], ['head', 'shaved', 'bald', 'except', 'scalp-lock'], ['tattoos'], [\"'s\", 'twenty-five'], ['seems', 'tall', 'muscled'], ['heavy', 'even', 'breathing'], [\"'ll\", 'learn', 'later']]}\n",
      "{2: [['man', 'uncas', 'last', 'mohicans']]}\n",
      "{1: [['flash', 'runs'], ['one', 'carries', 'flintlock', 'musket'], ['sweat', 'man', \"'s\", 'skin'], ['calico', 'shirt', 'gathered', 'waist', 'wampum', 'belt', 'small', 'white', 'beads', 'breechcloth'], ['wears', 'leggings', 'protect', 'legs'], ['long-handled', 'tomahawk', 'stuffed', 'belt']]}\n",
      "{0: [['another', 'part', 'forest', 'massive', 'war', 'club', 'day']]}\n"
     ]
    }
   ],
   "source": [
    "print_first_lines(screenplays_nonstop[10], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "del screenplays_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [['written', 'rhett', 'reese', 'paul', 'wernick', 'final', 'shooting', 'november']]}\n",
      "{1: [['black'], ['low', 'volume', 'tinny', 'speaker', 'juice', 'newton', \"'s\", 'angel', 'morning']]}\n",
      "{0: [['taxi', 'cab', 'morning']]}\n",
      "{1: [['deadpool', 'full', 'dress', 'reds', 'mask', 'quietly', 'fidgets', 'back', 'seat', 'taxi', 'cab', 'proceeds', 'along', 'city', 'freeway'], ['deadpool', 'adjusts', 'two', 'katanas', 'strapped', 'back'], ['rolls', 'windows'], ['tries', 'futilely', 'untwist', 'seatbelt', 'lunges', 'forward', 'locking'], ['rifles', 'tourist', 'booklet', 'tears', 'haunted', 'segway', 'tour', 'coupon'], ['cabbie', 'young', 'thin', 'brown', 'glances', 'back', 'forth', 'rear', 'view', 'road', 'rear', 'view']]}\n",
      "{2: [['kinda', 'lonesome', 'back']]}\n",
      "{2: [['little', 'help']]}\n",
      "{1: [['cabbie', 'grabs', 'deadpool', \"'s\", 'hand', 'pulls', 'front'], ['deadpool', \"'s\", 'head', 'rests', 'upside', 'bench', 'seat', 'maneuvers', 'legs'], ['cabbie', 'turns', 'helping', 'hand', 'handshake', 'turns', 'juice']]}\n",
      "{2: [['dopinder']]}\n",
      "{2: [['still', 'upside-down']]}\n",
      "{1: [['pool'], ['deadpool'], ['dopinder', 'remarkably', 'unaffected', 'lunatic', 'cab']]}\n"
     ]
    }
   ],
   "source": [
    "def remove_empties(dict_list):\n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            d[key] = [sent for sent in value if sent]\n",
    "    return dict_list\n",
    "\n",
    "# test on deadpool \n",
    "deadpool_cleaned = remove_empties(deadpool_nonstop)\n",
    "print_first_lines(deadpool_cleaned, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply all\n",
    "cleaned_screenplays = screenplays_nonstop.apply(remove_empties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [['fourth', 'draft', 'screenplay', 'james', 'baldwin', 'arnold', 'perl', 'spike', 'lee', 'based', 'autobiography', 'malcolm', 'told', 'alex', 'haley']]}\n",
      "{0: [['ext'], ['roxbury', 'street', 'war', 'years', 'day']]}\n",
      "{1: [['bright', 'sunny', 'day', 'crowded', 'street', 'black', 'side', 'boston'], ['people', 'kids', 'busy', 'things'], ['shorty', 'bops', 'way', 'street'], ['runty', 'dark', 'young', 'man', 'mission', 'smile', 'face'], ['wears', 'flamboyant', 'style', 'time', 'whole', 'zoot-suit', 'pegged', 'legs', 'wide', 'brim', 'hat', 'white', 'feather', 'stuck', 'hat', 'band']]}\n",
      "{0: [['ext'], ['street', 'day']]}\n",
      "{1: [['follow', 'shot'], ['shorty', 'dodges', 'crowd', 'packages'], ['smile', 'one', 'anticipation'], ['nods', 'pal', 'without', 'stopping', 'eyes', 'couple', 'chicks', 'dancing', 'street', 'dissuaded']]}\n",
      "{0: [['int'], ['barber', 'shop', 'day']]}\n",
      "{1: [['shorty', 'jacket', 'hat', 'sleeves', 'rolled'], ['like', 'surgeon', 'preparing', 'operation'], ['equipment', 'spread', 'table', 'lye', 'large', 'mason', 'jar', 'wooden', 'stirring', 'spoon', 'knife', 'eggs'], ['actions', 'character', 'ritual', 'thing', 'done', 'time-honored', 'fashion'], ['slices', 'potatoes', 'drops', 'thin', 'slices', 'mason', 'jar'], ['adds', 'water', 'makes', 'paste', 'starch'], ['behind', 'shorty', 'spirited', 'barbershop', 'conversation'], ['one', 'man', 'getting', 'haircut', 'two', 'others', 'watching', 'toomer', 'jason', 'one', 'behind', 'newspaper'], ['middle-aged', 'barber', 'cholly', 'talking']]}\n",
      "{2: [['hit', 'number', 'woman', \"n't\", 'good']]}\n",
      "{1: [['men', 'laugh']]}\n",
      "{1: [['angle', 'shorty', 'pries', 'open', 'lye', 'whiffs'], [\"'s\", 'good', 'strong'], ['pours', 'mason', 'jar', 'stirring', 'wooden', 'spoon'], ['cracks', 'eggs', 'mixture', 'stirs'], ['waits', 'fumes', 'rise', 'feels', 'outside', 'jar', 'gets', 'hot']]}\n"
     ]
    }
   ],
   "source": [
    "print_first_lines(cleaned_screenplays[12], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: [['fourth', 'draft', 'screenplay', 'james', 'baldwin', 'arnold', 'perl', 'spike', 'lee', 'based', 'autobiography', 'malcolm', 'told', 'alex', 'haley']]}\n",
      "{0: [[], ['roxbury', 'street', 'war', 'years', 'day']]}\n",
      "{1: [['bright', 'sunny', 'day', 'crowded', 'street', 'black', 'side', 'boston'], ['people', 'kids', 'busy', 'things'], ['shorty', 'bops', 'way', 'street'], ['runty', 'dark', 'young', 'man', 'mission', 'smile', 'face'], ['wears', 'flamboyant', 'style', 'time', 'whole', 'zoot-suit', 'pegged', 'legs', 'wide', 'brim', 'hat', 'white', 'feather', 'stuck', 'hat', 'band']]}\n",
      "{0: [[], ['street', 'day']]}\n",
      "{1: [['follow', 'shot'], ['shorty', 'dodges', 'crowd', 'packages'], ['smile', 'one', 'anticipation'], ['nods', 'pal', 'without', 'stopping', 'eyes', 'couple', 'chicks', 'dancing', 'street', 'dissuaded']]}\n",
      "{0: [[], ['barber', 'shop', 'day']]}\n",
      "{1: [['shorty', 'jacket', 'hat', 'sleeves', 'rolled'], ['like', 'surgeon', 'preparing', 'operation'], ['equipment', 'spread', 'table', 'lye', 'large', 'mason', 'jar', 'wooden', 'stirring', 'spoon', 'knife', 'eggs'], ['actions', 'character', 'ritual', 'thing', 'done', 'time-honored', 'fashion'], ['slices', 'potatoes', 'drops', 'thin', 'slices', 'mason', 'jar'], ['adds', 'water', 'makes', 'paste', 'starch'], ['behind', 'shorty', 'spirited', 'barbershop', 'conversation'], ['one', 'man', 'getting', 'haircut', 'two', 'others', 'watching', 'toomer', 'jason', 'one', 'behind', 'newspaper'], ['middle-aged', 'barber', 'cholly', 'talking']]}\n",
      "{2: [['hit', 'number', 'woman', \"n't\", 'good']]}\n",
      "{1: [['men', 'laugh']]}\n",
      "{1: [['angle', 'shorty', 'pries', 'open', 'lye', 'whiffs'], [\"'s\", 'good', 'strong'], ['pours', 'mason', 'jar', 'stirring', 'wooden', 'spoon'], ['cracks', 'eggs', 'mixture', 'stirs'], ['waits', 'fumes', 'rise', 'feels', 'outside', 'jar', 'gets', 'hot']]}\n"
     ]
    }
   ],
   "source": [
    "# remove stops again \n",
    "cleaned_screenplays = cleaned_screenplays.apply(remove_stops)\n",
    "print_first_lines(cleaned_screenplays[12], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_screenplays = cleaned_screenplays.apply(remove_empties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to a json\n",
    "cleaned_screenplays.to_json(f'{root_path}\\\\cleaned_screenplays.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- expand stops list \n",
    "- cut useless metadata if possible \n",
    "- lemmatize if possible \n",
    "- stem if not \n",
    "- reach a reasonable avg length target \n",
    "- apply phrases model\n",
    "- try word vectorization. The eventual output vectors should look like:\n",
    "{label code: sentence{ {v1}, {v2} etc.}}\n",
    "- Build a BERT annotator.  Use these annotations as supervision. \n",
    "- Run through NN pipeline.  Truncate aggressively.  Use samples only for training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
